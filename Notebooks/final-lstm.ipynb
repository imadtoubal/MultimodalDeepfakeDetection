{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to take features from XceptionNet and the the Spectrogram features prepared by the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a spectrogram feature\n",
    "spect_dir = '/home/jklc9f/data/dfdc/sample/train_spectrograms_part-5/real/name_video/name_video-000-24.pt'\n",
    "t = torch.load(spect_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check where the relevant information exists in the spectrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # t = t.numpy()\n",
    "# plt.imshow(t.transpose(), aspect='auto', origin='bottom', cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataset object for concatenated xception features and spectrogram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FrimagenetDataset(Dataset):\n",
    "    '''\n",
    "    FrimageNet data set for concatenating XceptionNet Features and Spectrogram features\n",
    "    '''\n",
    "    def __init__(self, spectrogram_folder, xception_features_folder):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spectrogram_folder (string): Path to the csv file with annotations.\n",
    "            xception_features_folder (string): Directory with all the images.\n",
    "        \"\"\"\n",
    "        self.classification = []\n",
    "        self.encode_map = {\n",
    "            'real': 1,\n",
    "            'fake': 0\n",
    "        }\n",
    "        self.features = self.__get_feats(spectrogram_folder, xception_features_folder)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.classification[idx]\n",
    "\n",
    "    def __get_feats(self, spect_directory, xception_directory, seq_size=24, nfirst=25, max_spect_feats=700, max_xcept_feats=2048):\n",
    "        samples = []\n",
    "        labels = ['real', 'fake']\n",
    "        for label in labels:\n",
    "            xception_vidpaths = sorted([os.path.join(xception_directory, label, vid) for vid in os.listdir(os.path.join(xception_directory, label))])\n",
    "            spect_vidpaths = sorted([os.path.join(spect_directory, label, vid) for vid in os.listdir(os.path.join(spect_directory, label))])\n",
    "\n",
    "            for xcept_path, spect_path in zip(xception_vidpaths, spect_vidpaths):\n",
    "                # loops through the paths to the video labels of xception features and spectrogram features folders\n",
    "                sorted_vid_xcept = sorted(os.listdir(xcept_path))\n",
    "                sorted_vid_spect = sorted(os.listdir(spect_path))\n",
    "\n",
    "                for xcept_feat, spect_feat in zip(sorted_vid_xcept, sorted_vid_spect):\n",
    "                    # loops throught the individual files in each respective video_id folder for the xception features and spectrogram features\n",
    "                    if (xcept_feat != spect_feat):\n",
    "                        # the labels are not identical, so alignment is off. Return error\n",
    "                        print(f'{xcept_feat} != {spect_feat} ')\n",
    "                        # raise NonAligned\n",
    "\n",
    "                    if xcept_feat[-5:] == f'{seq_size}.pt':\n",
    "                        xcept = torch.load(os.path.join(xcept_path, xcept_feat))[:, :max_xcept_feats]\n",
    "                        spect = torch.load(os.path.join(spect_path, spect_feat))[:, :max_spect_feats]\n",
    "                        samples.append(torch.cat((xcept, spect), dim=-1))\n",
    "                        self.classification.append(torch.tensor(self.encode_map[label]))\n",
    "        self.classification = torch.stack(self.classification)\n",
    "        return torch.stack(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let us build our LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FrimageNet(nn.Module):\n",
    "    def __init__(self, feature_size, num_layers=2, hidden_dim=512, device='cuda'):\n",
    "        super(FrimageNet, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # input dim is 167, output 200\n",
    "        self.lstm = nn.LSTM(feature_size, hidden_dim,\n",
    "                            batch_first=True, num_layers=num_layers)\n",
    "        # fully connected\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.act = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 2)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "#         print(x.device, hidden[0].device)\n",
    "        y, hidden = self.lstm(x, hidden)    # returns the two outputs\n",
    "        y = y[:, -1, :]  # get only the last output\n",
    "        y = self.fc1(y)\n",
    "        y = self.fc2(y)\n",
    "        y = F.softmax(y, dim=1)\n",
    "\n",
    "        return y, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_dim).zero_().to(self.device),\n",
    "                  weight.new(self.num_layers, batch_size, self.hidden_dim).zero_().to(self.device))\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get an example of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([24, 24, 2748])"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "spectrogram_folder = r'J:\\reu\\code\\output\\spectrogram_features' \n",
    "xception_folder = r'J:\\reu\\code\\output\\xception_features'\n",
    "data = FrimagenetDataset(spectrogram_folder, xception_folder)\n",
    "data.features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We wanted the xception features, which are of size 2048, to be concatenated with the relevant data from spectrgram features, which, as defined by the FrimagenetDataset class, are the first 700 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test out our LSTM with this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "FrimageNet(\n  (lstm): LSTM(2748, 512, num_layers=2, batch_first=True)\n  (fc1): Linear(in_features=512, out_features=512, bias=True)\n  (act): Sigmoid()\n  (fc2): Linear(in_features=512, out_features=2, bias=True)\n  (softmax): Softmax(dim=None)\n)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "#initialize model\n",
    "net = FrimageNet(feature_size=2748) # feature size matches the last dimension of Dataset Features\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a function to simulate training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model, spectrogram_folder, xception_folder, loss_function, optimizer, epochs=100, batch_size=5, device='cuda'):\n",
    "    training_data = FrimagenetDataset(spectrogram_folder, xception_folder)\n",
    "    trainloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for h in hidden:\n",
    "        h = h.to(device)    \n",
    "    \n",
    "    print_every = 20\n",
    "    i = 0\n",
    "    losses = []\n",
    "    accs = []\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for inp, labels in trainloader:  # renamed sequence to inp because inp is a batch of sequences\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            inp = inp.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Step 2. Run our forward pass.\n",
    "            tag_scores, h = model(inp, hidden)\n",
    "\n",
    "            # Step 3. Compute the loss, gradients, and update the parameters by\n",
    "            # calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_acc += torch.mean((tag_scores.argmax(dim=1) == labels).float()).item()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % print_every == print_every-1:\n",
    "                print('[%d, %5d] loss: %.3f - acc: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / print_every, running_acc * 100 / print_every))\n",
    "                \n",
    "                losses.append(running_loss / print_every)\n",
    "                accs.append(running_acc * 100 / print_every)\n",
    "                \n",
    "                running_loss = 0.0\n",
    "                running_acc = 0.0\n",
    "            i += 1\n",
    "    return losses, accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this function, we need to define a lsot function and optimizer. Let's use Cross Entropy Loss and Adam, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now before we train, let's make sure everything is on the same device (cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "FrimageNet(\n  (lstm): LSTM(2748, 512, num_layers=2, batch_first=True)\n  (fc1): Linear(in_features=512, out_features=512, bias=True)\n  (act): Sigmoid()\n  (fc2): Linear(in_features=512, out_features=2, bias=True)\n  (softmax): Softmax(dim=None)\n)"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "loss_function.to(device)\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[5,    20] loss: 0.693 - acc: 50.000\n[10,    40] loss: 0.694 - acc: 51.000\n[15,    60] loss: 0.693 - acc: 54.000\n[20,    80] loss: 0.693 - acc: 53.000\n[25,   100] loss: 0.693 - acc: 50.000\n[30,   120] loss: 0.693 - acc: 49.000\n[35,   140] loss: 0.693 - acc: 51.000\n[40,   160] loss: 0.694 - acc: 50.000\n[45,   180] loss: 0.693 - acc: 53.000\n[50,   200] loss: 0.693 - acc: 49.000\n[55,   220] loss: 0.694 - acc: 45.000\n[60,   240] loss: 0.693 - acc: 48.000\n[65,   260] loss: 0.694 - acc: 46.000\n[70,   280] loss: 0.693 - acc: 48.000\n[75,   300] loss: 0.693 - acc: 54.000\n[80,   320] loss: 0.694 - acc: 49.000\n[85,   340] loss: 0.693 - acc: 50.000\n[90,   360] loss: 0.693 - acc: 52.000\n[95,   380] loss: 0.693 - acc: 50.000\n[100,   400] loss: 0.693 - acc: 51.000\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "([0.693226546049118,\n  0.6935099601745606,\n  0.6929204791784287,\n  0.6926646858453751,\n  0.6930759340524674,\n  0.6933588236570358,\n  0.6931680500507355,\n  0.6935722380876541,\n  0.6926412552595138,\n  0.6933632642030716,\n  0.6935172408819199,\n  0.69336057305336,\n  0.6936771124601364,\n  0.6931857109069824,\n  0.6927893221378326,\n  0.6935473531484604,\n  0.6931888729333877,\n  0.6930012255907059,\n  0.6932780593633652,\n  0.693035489320755],\n [50.00000149011612,\n  51.00000157952309,\n  54.00000140070915,\n  53.0000014603138,\n  50.00000134110451,\n  49.00000125169754,\n  51.00000128149986,\n  50.00000096857548,\n  53.00000101327896,\n  49.0000007301569,\n  45.00000096857548,\n  48.00000123679638,\n  46.000001057982445,\n  48.00000123679638,\n  54.000001326203346,\n  49.00000110268593,\n  50.0000012665987,\n  52.00000137090683,\n  50.00000111758709,\n  51.000001430511475])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "train(net, spectrogram_folder, xception_folder, loss_function, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that, with fake data and a super small sample, we have put together an LSTM!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('venv': conda)",
   "language": "python",
   "name": "python_defaultSpec_1595869349355"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}